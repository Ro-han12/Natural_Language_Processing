{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is fun!\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('fun', 'NN'), ('!', '.')]\n",
      "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is fun!\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)\n",
    "import spacy\n",
    "\n",
    "# Load the English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# Process the text with spaCy NER\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the named entities\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'negative']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Sample training data\n",
    "train_data = [\"This is a positive review\", \"This is a negative review\",\n",
    "              \"I really enjoyed the movie\", \"The movie was terrible\"]\n",
    "\n",
    "# Labels for the training data\n",
    "labels = [\"positive\", \"negative\", \"positive\", \"negative\"]\n",
    "\n",
    "# Create a pipeline for text classification\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, labels)\n",
    "\n",
    "# Sample test data\n",
    "test_data = [\"I loved the movie\", \"The movie was awful\"]\n",
    "\n",
    "# Predict the labels for test data\n",
    "predicted_labels = model.predict(test_data)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.6804217696077145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/k2rvfmz92ld8z0rgrcsbrnl40000gn/T/ipykernel_11735/3388213878.py:15: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = doc1.similarity(doc2)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample texts\n",
    "text1 = \"apple\"\n",
    "text2 = \"banana\"\n",
    "\n",
    "# Process the texts with spaCy\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "\n",
    "# Calculate similarity between the texts\n",
    "similarity = doc1.similarity(doc2)\n",
    "\n",
    "# Print the similarity score\n",
    "print(\"Similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/rohansridhar/miniforge3/envs/google/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/rohansridhar/miniforge3/envs/google/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rohansridhar/miniforge3/envs/google/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Downloading gensim-4.3.2-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install gensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m \u001b[39mimport\u001b[39;00m summarize\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Sample text\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNatural Language Processing (NLP) is a field \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mof artificial intelligence that focuses on the \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39minteraction between computers and humans \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/NLP/Natural_Language_Processing/codes.ipynb/codes.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mthrough natural language.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing (NLP) is a field \" \\\n",
    "       \"of artificial intelligence that focuses on the \" \\\n",
    "       \"interaction between computers and humans \" \\\n",
    "       \"through natural language.\"\n",
    "\n",
    "# Summarize the text\n",
    "summary = summarize(text)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Steve/NNP)\n",
      "  (PERSON Jobs/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  co-founder/NN\n",
      "  of/IN\n",
      "  (PERSON Apple/NNP Inc/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  on/IN\n",
      "  February/NNP\n",
      "  24/CD\n",
      "  ,/,\n",
      "  1955/CD\n",
      "  ,/,\n",
      "  in/IN\n",
      "  (GPE San/NNP Francisco/NNP)\n",
      "  ,/,\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample text\n",
    "text = \"Steve Jobs was the co-founder of Apple Inc. \" \\\n",
    "       \"He was born on February 24, 1955, in San Francisco, California.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Perform NER on each sentence\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    entities = nltk.chunk.ne_chunk(tagged_words)\n",
    "    print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
